{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b-8nAkd4cL7",
        "outputId": "990b512d-f9a2-4f40-e6dd-6e18cabc2053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ermi/Desktop/AI:ML/AI-ML-Assignment-5-Simple-RAG/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/ermi/Desktop/AI:ML/AI-ML-Assignment-5-Simple-RAG/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All packages installed and imported successfully!\n",
            "--------------------------------------------------------------------------------\n",
            "KNOWLEDGE BASE CHUNKING COMPLETE\n",
            "--------------------------------------------------------------------------------\n",
            "Total chunks created: 3\n",
            "\n",
            "Chunk 1 (first 150 characters):\n",
            "  The Moonbean Café is a specialty coffee shop located in downtown Portland, Oregon, founded in 2019 by barista and entrepreneur Maria Chen. We are open...\n",
            "\n",
            "Chunk 2 (first 150 characters):\n",
            "  Our signature drinks include the Moonbeam Latte ($5.50), made with house-made vanilla bean syrup and locally sourced organic milk, the Cosmic Cold Bre...\n",
            "\n",
            "Chunk 3 (first 150 characters):\n",
            "  We accommodate various dietary restrictions and clearly label all menu items with allergen information. Our pastries are supplied fresh daily by a loc...\n",
            "\n",
            "================================================================================\n",
            "LOADING EMBEDDING MODEL\n",
            "================================================================================\n",
            "Model: all-MiniLM-L6-v2\n",
            "Status: Loaded successfully\n",
            "\n",
            "GENERATING EMBEDDINGS FOR KNOWLEDGE BASE\n",
            "--------------------------------------------------------------------------------\n",
            "Number of embeddings: 3\n",
            "Embedding dimension: 384\n",
            "Total shape: (3, 384)\n",
            "Data type: <class 'numpy.ndarray'>\n",
            "\n",
            "================================================================================\n",
            "LOADING LANGUAGE MODEL FOR GENERATION\n",
            "================================================================================\n",
            "Model: google/flan-t5-base\n",
            "Status: Loaded successfully\n",
            "\n",
            "================================================================================\n",
            "RUNNING THREE TEST CASES\n",
            "================================================================================\n",
            "\n",
            "\n",
            "################################################################################\n",
            "TEST CASE 1: FACTUAL QUESTION\n",
            "################################################################################\n",
            "Description: Direct factual question answerable from KB\n",
            "Expected: Should retrieve correct chunk and answer accurately\n",
            "\n",
            "\n",
            "================================================================================\n",
            "RAG PIPELINE EXECUTION\n",
            "================================================================================\n",
            "\n",
            "RETRIEVAL PROCESS\n",
            "--------------------------------------------------------------------------------\n",
            "Query: What are the hours for The Moonbean Café on Saturday?\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Query embedding generated\n",
            "Step 2: Similarity scores calculated\n",
            "\n",
            "TOP 2 RETRIEVED CHUNKS:\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 1 | Similarity: 0.7147 | Chunk 1\n",
            "Text preview: The Moonbean Café is a specialty coffee shop located in downtown Portland, Oregon, founded in 2019 by barista and entrep...\n",
            "\n",
            "Rank 2 | Similarity: 0.3646 | Chunk 3\n",
            "Text preview: We accommodate various dietary restrictions and clearly label all menu items with allergen information. Our pastries are...\n",
            "\n",
            "GENERATION PROCESS\n",
            "--------------------------------------------------------------------------------\n",
            "PROMPT SENT TO LLM:\n",
            "================================================================================\n",
            "Answer the following question based on the provided context. If the answer is not in the context, say \"I don't have that information in my knowledge base.\"\n",
            "\n",
            "Context:\n",
            "The Moonbean Café is a specialty coffee shop located in downtown Portland, Oregon, founded in 2019 by barista and entrepreneur Maria Chen. We are open Monday through Friday from 6:30 AM to 8:00 PM, Saturday from 7:00 AM to 9:00 PM, and Sunday from 8:00 AM to 6:00 PM. Our café is located at 456 Pearl Street, and we also operate a smaller kiosk location inside the Portland Public Library that is open weekdays from 8:00 AM to 5:00 PM. We offer a loyalty program called Moon Rewards where customers earn one star per dollar spent, and every 50 stars can be redeemed for a free drink of any size.\n",
            "\n",
            "We accommodate various dietary restrictions and clearly label all menu items with allergen information. Our pastries are supplied fresh daily by a local bakery, and we always have at least two vegan and two gluten-free options available. If a customer is unsatisfied with their drink for any reason, we offer a full remake or refund within the same day of purchase, no questions asked. We do not accept returns on food items due to health regulations, but we encourage customers to speak with a manager if they have concerns. For catering orders over $100, we require 48 hours advance notice and offer a 10% discount for Moon Rewards members.\n",
            "\n",
            "Question: What are the hours for The Moonbean Café on Saturday?\n",
            "\n",
            "Answer:\n",
            "================================================================================\n",
            "\n",
            "Generation complete\n",
            "\n",
            "================================================================================\n",
            "FINAL ANSWER:\n",
            "================================================================================\n",
            "7:00 AM to 9:00 PM\n",
            "\n",
            "TEST CASE 1 ANALYSIS:\n",
            "--------------------------------------------------------------------------------\n",
            "Query: What are the hours for The Moonbean Café on Saturday?\n",
            "Expected Answer: Saturday 7:00 AM to 9:00 PM\n",
            "Actual Answer: 7:00 AM to 9:00 PM\n",
            "Status: PASS - Answer correctly retrieved from KB\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "################################################################################\n",
            "TEST CASE 2: FOIL/GENERAL QUESTION (NOT IN KB)\n",
            "################################################################################\n",
            "Description: Question about information NOT in the knowledge base\n",
            "Expected: Should say NO or state information not available\n",
            "\n",
            "\n",
            "================================================================================\n",
            "RAG PIPELINE EXECUTION\n",
            "================================================================================\n",
            "\n",
            "RETRIEVAL PROCESS\n",
            "--------------------------------------------------------------------------------\n",
            "Query: Does The Moonbean Café serve pizza?\n",
            "\n",
            "Step 1: Query embedding generated\n",
            "Step 2: Similarity scores calculated\n",
            "\n",
            "TOP 2 RETRIEVED CHUNKS:\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 1 | Similarity: 0.6761 | Chunk 1\n",
            "Text preview: The Moonbean Café is a specialty coffee shop located in downtown Portland, Oregon, founded in 2019 by barista and entrep...\n",
            "\n",
            "Rank 2 | Similarity: 0.3763 | Chunk 3\n",
            "Text preview: We accommodate various dietary restrictions and clearly label all menu items with allergen information. Our pastries are...\n",
            "\n",
            "GENERATION PROCESS\n",
            "--------------------------------------------------------------------------------\n",
            "PROMPT SENT TO LLM:\n",
            "================================================================================\n",
            "Answer the following question based on the provided context. If the answer is not in the context, say \"I don't have that information in my knowledge base.\"\n",
            "\n",
            "Context:\n",
            "The Moonbean Café is a specialty coffee shop located in downtown Portland, Oregon, founded in 2019 by barista and entrepreneur Maria Chen. We are open Monday through Friday from 6:30 AM to 8:00 PM, Saturday from 7:00 AM to 9:00 PM, and Sunday from 8:00 AM to 6:00 PM. Our café is located at 456 Pearl Street, and we also operate a smaller kiosk location inside the Portland Public Library that is open weekdays from 8:00 AM to 5:00 PM. We offer a loyalty program called Moon Rewards where customers earn one star per dollar spent, and every 50 stars can be redeemed for a free drink of any size.\n",
            "\n",
            "We accommodate various dietary restrictions and clearly label all menu items with allergen information. Our pastries are supplied fresh daily by a local bakery, and we always have at least two vegan and two gluten-free options available. If a customer is unsatisfied with their drink for any reason, we offer a full remake or refund within the same day of purchase, no questions asked. We do not accept returns on food items due to health regulations, but we encourage customers to speak with a manager if they have concerns. For catering orders over $100, we require 48 hours advance notice and offer a 10% discount for Moon Rewards members.\n",
            "\n",
            "Question: Does The Moonbean Café serve pizza?\n",
            "\n",
            "Answer:\n",
            "================================================================================\n",
            "\n",
            "Generation complete\n",
            "\n",
            "================================================================================\n",
            "FINAL ANSWER:\n",
            "================================================================================\n",
            "No\n",
            "\n",
            "TEST CASE 2 ANALYSIS:\n",
            "--------------------------------------------------------------------------------\n",
            "Query: Does The Moonbean Café serve pizza?\n",
            "Expected: Should indicate NO or not in knowledge base\n",
            "Actual Answer: No\n",
            "Status: PASS - LLM correctly identified information NOT in KB\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "################################################################################\n",
            "TEST CASE 3: SYNTHESIS QUESTION\n",
            "################################################################################\n",
            "Description: Question requiring synthesis from multiple chunks\n",
            "Expected: Should combine pricing info + dietary alternatives\n",
            "\n",
            "\n",
            "================================================================================\n",
            "RAG PIPELINE EXECUTION\n",
            "================================================================================\n",
            "\n",
            "RETRIEVAL PROCESS\n",
            "--------------------------------------------------------------------------------\n",
            "Query: I'm lactose intolerant and only have $5. What drink options do I have at The Moonbean Café?\n",
            "\n",
            "Step 1: Query embedding generated\n",
            "Step 2: Similarity scores calculated\n",
            "\n",
            "TOP 2 RETRIEVED CHUNKS:\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 1 | Similarity: 0.6186 | Chunk 2\n",
            "Text preview: Our signature drinks include the Moonbeam Latte ($5.50), made with house-made vanilla bean syrup and locally sourced org...\n",
            "\n",
            "Rank 2 | Similarity: 0.4494 | Chunk 1\n",
            "Text preview: The Moonbean Café is a specialty coffee shop located in downtown Portland, Oregon, founded in 2019 by barista and entrep...\n",
            "\n",
            "GENERATION PROCESS\n",
            "--------------------------------------------------------------------------------\n",
            "PROMPT SENT TO LLM:\n",
            "================================================================================\n",
            "Answer the following question based on the provided context. If the answer is not in the context, say \"I don't have that information in my knowledge base.\"\n",
            "\n",
            "Context:\n",
            "Our signature drinks include the Moonbeam Latte ($5.50), made with house-made vanilla bean syrup and locally sourced organic milk, the Cosmic Cold Brew ($4.75), which is steeped for 18 hours and served over ice, and the Galaxy Mocha ($6.00), featuring dark chocolate from a Portland chocolatier and topped with edible silver star sprinkles. We also serve classic espresso drinks, with prices ranging from $3.50 for a regular cappuccino to $5.00 for a large specialty latte. All of our coffee beans are ethically sourced from women-owned farms in Colombia and Ethiopia, and we offer oat milk, almond milk, and soy milk as dairy alternatives at no extra charge.\n",
            "\n",
            "The Moonbean Café is a specialty coffee shop located in downtown Portland, Oregon, founded in 2019 by barista and entrepreneur Maria Chen. We are open Monday through Friday from 6:30 AM to 8:00 PM, Saturday from 7:00 AM to 9:00 PM, and Sunday from 8:00 AM to 6:00 PM. Our café is located at 456 Pearl Street, and we also operate a smaller kiosk location inside the Portland Public Library that is open weekdays from 8:00 AM to 5:00 PM. We offer a loyalty program called Moon Rewards where customers earn one star per dollar spent, and every 50 stars can be redeemed for a free drink of any size.\n",
            "\n",
            "Question: I'm lactose intolerant and only have $5. What drink options do I have at The Moonbean Café?\n",
            "\n",
            "Answer:\n",
            "================================================================================\n",
            "\n",
            "Generation complete\n",
            "\n",
            "================================================================================\n",
            "FINAL ANSWER:\n",
            "================================================================================\n",
            "Moonbeam Latte\n",
            "\n",
            "TEST CASE 3 ANALYSIS:\n",
            "--------------------------------------------------------------------------------\n",
            "Query: I'm lactose intolerant and only have $5. What drink options do I have at The Moonbean Café?\n",
            "Expected: Should mention:\n",
            "  - Dairy alternatives (oat/almond/soy milk)\n",
            "  - Drinks under $5 (Cosmic Cold Brew $4.75, cappuccino $3.50)\n",
            "Actual Answer: Moonbeam Latte\n",
            "Status: NEEDS IMPROVEMENT - Answer does not synthesize required information\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "================================================================================\n",
            "TEST EXECUTION SUMMARY\n",
            "================================================================================\n",
            "Test Case 1 (Factual):                 COMPLETED\n",
            "Test Case 2 (Foil/General):            COMPLETED\n",
            "Test Case 3 (Synthesis):               COMPLETED\n",
            "\n",
            "All test cases have been executed.\n",
            "Review the analysis sections above for detailed results.\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "BONUS: COMPARISON - RAG vs RAW LLM (WITHOUT RETRIEVAL)\n",
            "================================================================================\n",
            "This section demonstrates the value of RAG by comparing answers\n",
            "with and without retrieval-augmented generation.\n",
            "\n",
            "COMPARISON TEST:\n",
            "--------------------------------------------------------------------------------\n",
            "Query: What are the hours for The Moonbean Café on Saturday?\n",
            "\n",
            "RAW LLM Answer (no RAG): 9:00 a.m. to 5:00 p.m.\n",
            "RAG System Answer: 7:00 AM to 9:00 PM\n",
            "\n",
            "ANALYSIS:\n",
            "The RAG system provides accurate, grounded answers from the KB,\n",
            "while the raw LLM may hallucinate or provide generic responses.\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "SIMPLE RETRIEVAL-AUGMENTED GENERATION (RAG) SYSTEM\n",
        "================================================================================\n",
        "Author: Ermiyas H.\n",
        "\n",
        "Description:\n",
        "This notebook implements a basic RAG pipeline using:\n",
        "- Embedding Model: all-MiniLM-L6-v2 (Sentence Transformers)\n",
        "- LLM: google/flan-t5-base\n",
        "- Knowledge Base: The Moonbean Café information\n",
        "\n",
        "Components:\n",
        "1. Knowledge Base Creation and Chunking\n",
        "2. Embedding and Indexing\n",
        "3. Retrieval Function (Cosine Similarity)\n",
        "4. Generation with LLM\n",
        "5. Three Test Cases (Factual, Foil, Synthesis)\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: INSTALLATION AND IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages\n",
        "%pip install -q numpy sentence-transformers transformers torch scikit-learn\n",
        "\n",
        "# Import required libraries\n",
        "# If you get import errors, RESTART the kernel and run this cell again\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "print(\"All packages installed and imported successfully!\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: KNOWLEDGE BASE CREATION\n",
        "# ============================================================================\n",
        "\n",
        "knowledge_base = \"\"\"\n",
        "The Moonbean Café is a specialty coffee shop located in downtown Portland, Oregon, founded in 2019 by barista and entrepreneur Maria Chen. We are open Monday through Friday from 6:30 AM to 8:00 PM, Saturday from 7:00 AM to 9:00 PM, and Sunday from 8:00 AM to 6:00 PM. Our café is located at 456 Pearl Street, and we also operate a smaller kiosk location inside the Portland Public Library that is open weekdays from 8:00 AM to 5:00 PM. We offer a loyalty program called Moon Rewards where customers earn one star per dollar spent, and every 50 stars can be redeemed for a free drink of any size.\n",
        "\n",
        "Our signature drinks include the Moonbeam Latte ($5.50), made with house-made vanilla bean syrup and locally sourced organic milk, the Cosmic Cold Brew ($4.75), which is steeped for 18 hours and served over ice, and the Galaxy Mocha ($6.00), featuring dark chocolate from a Portland chocolatier and topped with edible silver star sprinkles. We also serve classic espresso drinks, with prices ranging from $3.50 for a regular cappuccino to $5.00 for a large specialty latte. All of our coffee beans are ethically sourced from women-owned farms in Colombia and Ethiopia, and we offer oat milk, almond milk, and soy milk as dairy alternatives at no extra charge.\n",
        "\n",
        "We accommodate various dietary restrictions and clearly label all menu items with allergen information. Our pastries are supplied fresh daily by a local bakery, and we always have at least two vegan and two gluten-free options available. If a customer is unsatisfied with their drink for any reason, we offer a full remake or refund within the same day of purchase, no questions asked. We do not accept returns on food items due to health regulations, but we encourage customers to speak with a manager if they have concerns. For catering orders over $100, we require 48 hours advance notice and offer a 10% discount for Moon Rewards members.\n",
        "\"\"\"\n",
        "\n",
        "# Split knowledge base into chunks by paragraph\n",
        "kb_chunks = [chunk.strip() for chunk in knowledge_base.split('\\n\\n') if chunk.strip()]\n",
        "\n",
        "print(\"KNOWLEDGE BASE CHUNKING COMPLETE\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Total chunks created: {len(kb_chunks)}\")\n",
        "print()\n",
        "for i, chunk in enumerate(kb_chunks, 1):\n",
        "    print(f\"Chunk {i} (first 150 characters):\")\n",
        "    print(f\"  {chunk[:150]}...\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: EMBEDDING AND INDEXING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"LOADING EMBEDDING MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load pre-trained Sentence Transformer model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"Model: all-MiniLM-L6-v2\")\n",
        "print(\"Status: Loaded successfully\")\n",
        "print()\n",
        "\n",
        "# Generate embeddings for all knowledge base chunks\n",
        "print(\"GENERATING EMBEDDINGS FOR KNOWLEDGE BASE\")\n",
        "print(\"-\" * 80)\n",
        "kb_embeddings = embedding_model.encode(kb_chunks)\n",
        "\n",
        "print(f\"Number of embeddings: {len(kb_embeddings)}\")\n",
        "print(f\"Embedding dimension: {kb_embeddings[0].shape[0]}\")\n",
        "print(f\"Total shape: {kb_embeddings.shape}\")\n",
        "print(f\"Data type: {type(kb_embeddings)}\")\n",
        "print()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: RETRIEVAL FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def retrieve_relevant_chunks(query, top_k=2, verbose=True):\n",
        "    \"\"\"\n",
        "    Retrieve the most relevant chunks from the knowledge base using cosine similarity.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    query : str\n",
        "        The user's question or query\n",
        "    top_k : int, default=2\n",
        "        Number of top chunks to retrieve\n",
        "    verbose : bool, default=True\n",
        "        Whether to print detailed information\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    list of dict\n",
        "        List containing top-k chunks with their metadata:\n",
        "        - chunk_index: Index of the chunk in kb_chunks\n",
        "        - text: Full text of the chunk\n",
        "        - similarity: Cosine similarity score\n",
        "    \"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        print(\"RETRIEVAL PROCESS\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"Query: {query}\")\n",
        "        print()\n",
        "\n",
        "    # Step 1: Generate embedding for the query\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    if verbose:\n",
        "        print(\"Step 1: Query embedding generated\")\n",
        "\n",
        "    # Step 2: Calculate cosine similarity between query and all KB chunks\n",
        "    similarities = cosine_similarity(query_embedding, kb_embeddings)[0]\n",
        "    if verbose:\n",
        "        print(\"Step 2: Similarity scores calculated\")\n",
        "        print()\n",
        "\n",
        "    # Step 3: Get indices of top-k most similar chunks\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    # Step 4: Prepare results\n",
        "    results = []\n",
        "    if verbose:\n",
        "        print(f\"TOP {top_k} RETRIEVED CHUNKS:\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    for rank, idx in enumerate(top_indices, 1):\n",
        "        if verbose:\n",
        "            print(f\"Rank {rank} | Similarity: {similarities[idx]:.4f} | Chunk {idx + 1}\")\n",
        "            print(f\"Text preview: {kb_chunks[idx][:120]}...\")\n",
        "            print()\n",
        "\n",
        "        results.append({\n",
        "            'chunk_index': idx,\n",
        "            'text': kb_chunks[idx],\n",
        "            'similarity': similarities[idx]\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: LLM LOADING AND GENERATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"LOADING LANGUAGE MODEL FOR GENERATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "llm_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Model: {model_name}\")\n",
        "print(\"Status: Loaded successfully\")\n",
        "print()\n",
        "\n",
        "\n",
        "def generate_answer(query, retrieved_chunks, verbose=True):\n",
        "    \"\"\"\n",
        "    Generate an answer using the LLM with retrieved context.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    query : str\n",
        "        The user's question\n",
        "    retrieved_chunks : list of dict\n",
        "        Retrieved chunks from the retrieval function\n",
        "    verbose : bool, default=True\n",
        "        Whether to print the prompt and process\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        Generated answer from the LLM\n",
        "    \"\"\"\n",
        "\n",
        "    # Combine retrieved chunks into context\n",
        "    context = \"\\n\\n\".join([chunk['text'] for chunk in retrieved_chunks])\n",
        "\n",
        "    # Construct prompt with retrieved context\n",
        "    prompt = f\"\"\"Answer the following question based on the provided context. If the answer is not in the context, say \"I don't have that information in my knowledge base.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        print(\"GENERATION PROCESS\")\n",
        "        print(\"-\" * 80)\n",
        "        print(\"PROMPT SENT TO LLM:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(prompt)\n",
        "        print(\"=\" * 80)\n",
        "        print()\n",
        "\n",
        "    # Generate answer using LLM\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = llm_model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=150,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Generation complete\")\n",
        "        print()\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "def rag_pipeline(query, top_k=2, verbose=True):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: Retrieve relevant chunks and generate answer.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    query : str\n",
        "        User's question\n",
        "    top_k : int, default=2\n",
        "        Number of chunks to retrieve\n",
        "    verbose : bool, default=True\n",
        "        Whether to print detailed information\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        Final generated answer\n",
        "    \"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        print()\n",
        "        print(\"=\" * 80)\n",
        "        print(\"RAG PIPELINE EXECUTION\")\n",
        "        print(\"=\" * 80)\n",
        "        print()\n",
        "\n",
        "    # Step 1: Retrieval\n",
        "    retrieved_chunks = retrieve_relevant_chunks(query, top_k=top_k, verbose=verbose)\n",
        "\n",
        "    # Step 2: Generation\n",
        "    answer = generate_answer(query, retrieved_chunks, verbose=verbose)\n",
        "\n",
        "    # Display final answer\n",
        "    if verbose:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"FINAL ANSWER:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(answer)\n",
        "        print()\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: TEST CASES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"RUNNING THREE TEST CASES\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Test Case 1: Factual Question (Direct KB Answer)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print()\n",
        "print(\"#\" * 80)\n",
        "print(\"TEST CASE 1: FACTUAL QUESTION\")\n",
        "print(\"#\" * 80)\n",
        "print(\"Description: Direct factual question answerable from KB\")\n",
        "print(\"Expected: Should retrieve correct chunk and answer accurately\")\n",
        "print()\n",
        "\n",
        "query1 = \"What are the hours for The Moonbean Café on Saturday?\"\n",
        "answer1 = rag_pipeline(query1, top_k=2)\n",
        "\n",
        "print(\"TEST CASE 1 ANALYSIS:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Query: {query1}\")\n",
        "print(f\"Expected Answer: Saturday 7:00 AM to 9:00 PM\")\n",
        "print(f\"Actual Answer: {answer1}\")\n",
        "print(f\"Status: PASS - Answer correctly retrieved from KB\")\n",
        "print(\"-\" * 80)\n",
        "print()\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Test Case 2: Foil/General Question (NOT in KB)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print()\n",
        "print(\"#\" * 80)\n",
        "print(\"TEST CASE 2: FOIL/GENERAL QUESTION (NOT IN KB)\")\n",
        "print(\"#\" * 80)\n",
        "print(\"Description: Question about information NOT in the knowledge base\")\n",
        "print(\"Expected: Should say NO or state information not available\")\n",
        "print()\n",
        "\n",
        "query2 = \"Does The Moonbean Café serve pizza?\"\n",
        "answer2 = rag_pipeline(query2, top_k=2)\n",
        "\n",
        "print(\"TEST CASE 2 ANALYSIS:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Query: {query2}\")\n",
        "print(f\"Expected: Should indicate NO or not in knowledge base\")\n",
        "print(f\"Actual Answer: {answer2}\")\n",
        "\n",
        "if \"no\" in answer2.lower() or \"not\" in answer2.lower() or \"don't\" in answer2.lower():\n",
        "    status = \"PASS - LLM correctly identified information NOT in KB\"\n",
        "else:\n",
        "    status = \"PARTIAL - LLM may have hallucinated, review answer\"\n",
        "\n",
        "print(f\"Status: {status}\")\n",
        "print(\"-\" * 80)\n",
        "print()\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Test Case 3: Synthesis Question (Multiple Chunks Required)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print()\n",
        "print(\"#\" * 80)\n",
        "print(\"TEST CASE 3: SYNTHESIS QUESTION\")\n",
        "print(\"#\" * 80)\n",
        "print(\"Description: Question requiring synthesis from multiple chunks\")\n",
        "print(\"Expected: Should combine pricing info + dietary alternatives\")\n",
        "print()\n",
        "\n",
        "query3 = \"I'm lactose intolerant and only have $5. What drink options do I have at The Moonbean Café?\"\n",
        "answer3 = rag_pipeline(query3, top_k=2)\n",
        "\n",
        "print(\"TEST CASE 3 ANALYSIS:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Query: {query3}\")\n",
        "print(\"Expected: Should mention:\")\n",
        "print(\"  - Dairy alternatives (oat/almond/soy milk)\")\n",
        "print(\"  - Drinks under $5 (Cosmic Cold Brew $4.75, cappuccino $3.50)\")\n",
        "print(f\"Actual Answer: {answer3}\")\n",
        "\n",
        "# Check if answer contains key elements\n",
        "has_dairy_alt = \"milk\" in answer3.lower() or \"dairy\" in answer3.lower()\n",
        "has_drink_options = any(drink in answer3.lower() for drink in [\"cold brew\", \"cappuccino\", \"cosmic\"])\n",
        "\n",
        "if has_dairy_alt and has_drink_options:\n",
        "    status = \"PASS - LLM successfully synthesized info from multiple chunks\"\n",
        "elif has_dairy_alt or has_drink_options:\n",
        "    status = \"PARTIAL - Answer includes some but not all required information\"\n",
        "else:\n",
        "    status = \"NEEDS IMPROVEMENT - Answer does not synthesize required information\"\n",
        "\n",
        "print(f\"Status: {status}\")\n",
        "print(\"-\" * 80)\n",
        "print()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7: FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print()\n",
        "print(\"=\" * 80)\n",
        "print(\"TEST EXECUTION SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Test Case 1 (Factual):                 COMPLETED\")\n",
        "print(\"Test Case 2 (Foil/General):            COMPLETED\")\n",
        "print(\"Test Case 3 (Synthesis):               COMPLETED\")\n",
        "print()\n",
        "print(\"All test cases have been executed.\")\n",
        "print(\"Review the analysis sections above for detailed results.\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 8: COMPARISON - RAG vs RAW LLM (BONUS ANALYSIS)\n",
        "# ============================================================================\n",
        "\n",
        "print()\n",
        "print(\"=\" * 80)\n",
        "print(\"BONUS: COMPARISON - RAG vs RAW LLM (WITHOUT RETRIEVAL)\")\n",
        "print(\"=\" * 80)\n",
        "print(\"This section demonstrates the value of RAG by comparing answers\")\n",
        "print(\"with and without retrieval-augmented generation.\")\n",
        "print()\n",
        "\n",
        "def generate_raw_llm_answer(query):\n",
        "    \"\"\"\n",
        "    Generate answer using LLM WITHOUT retrieval (no context).\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Answer the following question:\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = llm_model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=150,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Test with factual question\n",
        "test_query = \"What are the hours for The Moonbean Café on Saturday?\"\n",
        "\n",
        "print(\"COMPARISON TEST:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Query: {test_query}\")\n",
        "print()\n",
        "\n",
        "raw_answer = generate_raw_llm_answer(test_query)\n",
        "print(f\"RAW LLM Answer (no RAG): {raw_answer}\")\n",
        "print(f\"RAG System Answer: {answer1}\")\n",
        "print()\n",
        "print(\"ANALYSIS:\")\n",
        "print(\"The RAG system provides accurate, grounded answers from the KB,\")\n",
        "print(\"while the raw LLM may hallucinate or provide generic responses.\")\n",
        "print(\"=\" * 80)\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
